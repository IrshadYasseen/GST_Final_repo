{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv files combined CSV.\n",
    "merged_train=pd.read_csv(\"merged_train.csv\")\n",
    "merged_test=pd.read_csv(\"merged_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the duplicates.\n",
    "merged_train.drop_duplicates(inplace=True)\n",
    "merged_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         0\n",
       "Column0            2\n",
       "Column1            0\n",
       "Column2            0\n",
       "Column3        42234\n",
       "Column4        42710\n",
       "Column5        55659\n",
       "Column6         1234\n",
       "Column7            0\n",
       "Column8         1234\n",
       "Column9       243853\n",
       "Column10           0\n",
       "Column11           0\n",
       "Column12           0\n",
       "Column13           0\n",
       "Column14      121679\n",
       "Column15        5485\n",
       "Column16           0\n",
       "Column17           0\n",
       "Column18           0\n",
       "Column19           0\n",
       "Column20           0\n",
       "Column21           0\n",
       "target             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         0\n",
       "Column0            9\n",
       "Column1            0\n",
       "Column2            0\n",
       "Column3       126303\n",
       "Column4       127710\n",
       "Column5       167180\n",
       "Column6         3850\n",
       "Column7            0\n",
       "Column8         3850\n",
       "Column9       732137\n",
       "Column10           0\n",
       "Column11           0\n",
       "Column12           0\n",
       "Column13           0\n",
       "Column14      365703\n",
       "Column15       16456\n",
       "Column16           0\n",
       "Column17           0\n",
       "Column18           0\n",
       "Column19           0\n",
       "Column20           0\n",
       "Column21           0\n",
       "target             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column0\n",
       "0.0     210054\n",
       "2.0      24616\n",
       "1.0      18312\n",
       "5.0       2117\n",
       "4.0       2112\n",
       "6.0       2007\n",
       "7.0       1401\n",
       "3.0        549\n",
       "9.0        159\n",
       "8.0        143\n",
       "11.0       112\n",
       "12.0        54\n",
       "14.0        30\n",
       "15.0        22\n",
       "16.0         7\n",
       "10.0         5\n",
       "13.0         5\n",
       "18.0         3\n",
       "17.0         2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test.Column0.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns based on certain conditions.\n",
    "merged_train.drop(columns=['Unnamed: 0','Column10','Column11','Column13','Column16','Column9','Column14'],inplace=True)\n",
    "merged_test.drop(columns=['Unnamed: 0','Column10','Column11','Column13','Column16','Column9','Column14'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2330/2551480196.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_train['Column0'].fillna(mode_value, inplace=True)\n",
      "/tmp/ipykernel_2330/2551480196.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_test['Column0'].fillna(mode_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "mode_value = merged_train['Column0'].mode()[0]\n",
    "\n",
    "# Fill missing values in 'Column0' with the mode\n",
    "merged_train['Column0'].fillna(mode_value, inplace=True)\n",
    "merged_test['Column0'].fillna(mode_value, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling null values.\n",
    "merged_train=merged_train.loc[~(merged_train['Column0']).isnull()]\n",
    "merged_test=merged_test.loc[~(merged_test['Column0'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column0          0\n",
       "Column1          0\n",
       "Column2          0\n",
       "Column3     126303\n",
       "Column4     127710\n",
       "Column5     167180\n",
       "Column6       3850\n",
       "Column7          0\n",
       "Column8       3850\n",
       "Column12         0\n",
       "Column15     16456\n",
       "Column17         0\n",
       "Column18         0\n",
       "Column19         0\n",
       "Column20         0\n",
       "Column21         0\n",
       "target           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Median impute the columns 15 and 5 due to their skewness.\n",
    "merged_train['Column15']=merged_train['Column15'].fillna(merged_train['Column15'].median())\n",
    "merged_test['Column15']=merged_test['Column15'].fillna(merged_test['Column15'].median())\n",
    "\n",
    "merged_train['Column5']=merged_train['Column5'].fillna(merged_train['Column5'].median())\n",
    "merged_test['Column5']=merged_test['Column5'].fillna(merged_test['Column5'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train=merged_train.loc[~(merged_train['Column6'].isnull() & merged_train['Column8'].isnull())]\n",
    "merged_test=merged_test.loc[~(merged_test['Column6'].isnull() & merged_test['Column8'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing column3 and column4 usinf MICE, cuz these two are linearly co-related to each other.\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(random_state=100, max_iter=250)\n",
    "\n",
    "mice_imputed_test=imputer.fit_transform(merged_test[['Column3','Column4']])\n",
    "mice_imputed_train=imputer.fit_transform(merged_train[['Column3','Column4']])\n",
    "\n",
    "merged_test[['Column3','Column4']]=mice_imputed_test\n",
    "merged_train[['Column3','Column4']]=mice_imputed_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def Scalerr(merged_train, merged_test, cols_to_scale):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on the training set only\n",
    "    scaler.fit(merged_train[cols_to_scale])\n",
    "    \n",
    "    # Transform both the training and test set\n",
    "    merged_train[cols_to_scale] = pd.DataFrame(scaler.transform(merged_train[cols_to_scale]), \n",
    "                                               columns=cols_to_scale, \n",
    "                                               index=merged_train.index)\n",
    "    \n",
    "    merged_test[cols_to_scale] = pd.DataFrame(scaler.transform(merged_test[cols_to_scale]), \n",
    "                                              columns=cols_to_scale, \n",
    "                                              index=merged_test.index)\n",
    "\n",
    "# Example usage:\n",
    "cols_to_scale = ['Column1', 'Column2']  # Add as many columns as you want to scale\n",
    "Scalerr(merged_train, merged_test, cols_to_scale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1)  # You can choose the number of components based on your needs\n",
    "features_train = merged_train[['Column3','Column4']]\n",
    "features_test=merged_test[['Column3','Column4']]\n",
    "merged_train['Col_3&4'] = pca.fit_transform(features_train)\n",
    "merged_test['Col_3&4']=pca.fit_transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train = merged_train.copy()\n",
    "transformed_test=merged_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column5     27.597343\n",
      "Column6      1.461022\n",
      "Column7      8.738096\n",
      "Column8      0.252854\n",
      "Column15    48.329992\n",
      "Column0      1.524206\n",
      "Column17     6.286120\n",
      "Column18     2.208732\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "negative_skewed_columns = ['Column5','Column6', 'Column7', 'Column8', 'Column15', 'Column0','Column17','Column18']\n",
    "       \n",
    "def yeo_johnson_transform(df, columns):\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    for col in columns:\n",
    "        df[col] = pt.fit_transform(df[[col]])  # Transform the original column directly\n",
    "    return df\n",
    "\n",
    "transformed_train = yeo_johnson_transform(transformed_train, negative_skewed_columns)\n",
    "\n",
    "# Optionally, you can print the transformed data or check the skewness\n",
    "# print(transformed_train[positive_skewed_columns].skew())\n",
    "print(transformed_train[negative_skewed_columns].skew())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns and their skewness types (to decide the transformation technique)\n",
    "\n",
    "\n",
    "negative_skewed_columns = ['Column5','Column6', 'Column7', 'Column8','Column15','Column0','Column17','Column18']\n",
    "\n",
    "\n",
    "# 2. Apply Yeo-Johnson Transformation for Highly Negative or Mixed Skewed Data\n",
    "def yeo_johnson_transform(df, columns):\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    for col in columns:\n",
    "        df[col] = pt.fit_transform(df[[col]])  # Transform the original column directly\n",
    "    return df\n",
    "\n",
    "transformed_test = yeo_johnson_transform(transformed_test, negative_skewed_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Function to winsorize outliers for multiple columns\n",
    "def winsorize_outliers(df, columns, lower_percentile=0.05, upper_percentile=0.95):\n",
    "    \n",
    "    for col in columns:\n",
    "        non_null_mask = df[col].notnull()\n",
    "        df.loc[non_null_mask, col] = winsorize(df.loc[non_null_mask, col], limits=(lower_percentile, 1 - upper_percentile))\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'data' is your DataFrame and you want to winsorize 'Column0', 'Column3', and 'Column5'\n",
    "columns_to_winsorize = ['Column0','Column5','Column6', 'Column7', 'Column8','Column15','Column17',\n",
    "       'Column18']\n",
    "transformed_train = winsorize_outliers(transformed_train, columns_to_winsorize, lower_percentile=0.01, upper_percentile=0.99)\n",
    "transformed_test = winsorize_outliers(transformed_test, columns_to_winsorize, lower_percentile=0.01, upper_percentile=0.99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_train=transformed_train\n",
    "ready_test=transformed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post Transformation.\n",
    "\n",
    "c = ['Column0','Column5',\n",
    "       'Column6', 'Column7', 'Column8','Column15', 'Column17',\n",
    "       'Column18','Col_3&4']\n",
    "\n",
    "scaler = RobustScaler() \n",
    "\n",
    "ready_train[c] = scaler.fit_transform(ready_train[c])\n",
    "ready_test[c] = scaler.fit_transform(ready_test[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=ready_train.drop(columns=['target','Column3','Column4'],axis=1)\n",
    "y_train=ready_train['target']\n",
    "\n",
    "X_test=ready_test.drop(columns=['target','Column3','Column4'],axis=1)\n",
    "y_test=ready_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_important_cols=['Column0','Column2', 'Column5', 'Column6', 'Column7',\n",
    "       'Column8','Column15', 'Column17','Col_3&4']\n",
    "#least important columns foud by feature importance. to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "least_important_features = X_train[least_important_cols]\n",
    "\n",
    "# Step 1: Apply KMeans clustering on the least important features\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)  # Adjust number of clusters as needed\n",
    "kmeans.fit(least_important_features)\n",
    "\n",
    "# Step 2: Compute the distance of each data point to each cluster centroid\n",
    "distances = kmeans.transform(least_important_features)  # This gives an array with shape (n_samples, n_clusters)\n",
    "\n",
    "# Step 3: Add the distance matrix as new features to the main dataset\n",
    "# Each column represents the distance to one cluster centroid\n",
    "for i in range(distances.shape[1]):\n",
    "    X_train[f'Distance_to_Cluster_{i}'] = distances[:, i]\n",
    "\n",
    "test_feature_scaled=X_test[least_important_cols]\n",
    "# Compute the distance of test data points to each cluster centroid\n",
    "test_distances = kmeans.transform(test_feature_scaled)  # This will give (n_test_samples, n_clusters) matrix\n",
    "\n",
    "# Add the distances as new features to the test dataset\n",
    "for i in range(test_distances.shape[1]):\n",
    "    X_test[f'Distance_to_Cluster_{i}'] = test_distances[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmeans_model.pkl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(kmeans,'kmeans_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
